{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9464c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vulcan/miniconda3/envs/ml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d27e8045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned_aadhaar_dataset.csv...\n",
      "✓ Data loaded successfully: (219091, 7)\n",
      "✓ Data sorted by geography and date\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Data\n",
    "print(\"Loading cleaned_aadhaar_dataset.csv...\")\n",
    "try:\n",
    "    df = pd.read_csv('/home/vulcan/Abhay/Projects/ADA/Dataset/cleaned_aadhaar_dataset.csv')\n",
    "    print(f\"✓ Data loaded successfully: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'cleaned_aadhaar_dataset.csv' not found.\")\n",
    "    raise\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Sort by date for time-series features (CRITICAL for causality)\n",
    "df = df.sort_values(['state', 'district', 'pincode', 'date']).reset_index(drop=True)\n",
    "print(f\"✓ Data sorted by geography and date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbb4c5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporal features...\n",
      "✓ Created 15 temporal features\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Temporal Features\n",
    "print(\"Creating temporal features...\")\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "df['week_of_year'] = df['date'].dt.isocalendar().week.astype('int64')\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "df['days_in_month'] = df['date'].dt.days_in_month\n",
    "df['day_sin'] = np.sin(2 * np.pi * df['day'] / 31)\n",
    "df['day_cos'] = np.cos(2 * np.pi * df['day'] / 31)\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "print(f\"✓ Created {15} temporal features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d4d4b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating demographic features...\n",
      "✓ Created 7 demographic features\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Demographic Features\n",
    "print(\"Creating demographic features...\")\n",
    "df['total_enrollments'] = df['age_0_5'] + df['age_5_17'] + df['age_18_greater']\n",
    "df['child_ratio'] = df['age_0_5'] / (df['total_enrollments'] + 1e-10)\n",
    "df['youth_ratio'] = df['age_5_17'] / (df['total_enrollments'] + 1e-10)\n",
    "df['adult_ratio'] = df['age_18_greater'] / (df['total_enrollments'] + 1e-10)\n",
    "df['child_adult_ratio'] = df['age_0_5'] / (df['age_18_greater'] + 1e-10)\n",
    "df['youth_adult_ratio'] = df['age_5_17'] / (df['age_18_greater'] + 1e-10)\n",
    "df['dependent_ratio'] = (df['age_0_5'] + df['age_5_17']) / (df['age_18_greater'] + 1e-10)\n",
    "\n",
    "print(f\"✓ Created {7} demographic features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655e5b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lag features...\n",
      "✓ Created 32 lag/rolling features\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Lag Features (Strictly Causal)\n",
    "print(\"Creating lag features...\")\n",
    "group_cols = ['state', 'district', 'pincode']\n",
    "lag_features_created = 0\n",
    "\n",
    "for col in ['age_0_5', 'age_5_17', 'age_18_greater', 'total_enrollments']:\n",
    "    df[f'{col}_lag1'] = df.groupby(group_cols)[col].shift(1)\n",
    "    df[f'{col}_lag7'] = df.groupby(group_cols)[col].shift(7)\n",
    "    df[f'{col}_lag30'] = df.groupby(group_cols)[col].shift(30)\n",
    "    \n",
    "    df[f'{col}_pct_change_1d'] = df.groupby(group_cols)[col].pct_change(1)\n",
    "    df[f'{col}_pct_change_7d'] = df.groupby(group_cols)[col].pct_change(7)\n",
    "    \n",
    "    df[f'{col}_rolling_mean_7d'] = df.groupby(group_cols)[col].transform(\n",
    "        lambda x: x.rolling(7, min_periods=1).mean()\n",
    "    )\n",
    "    df[f'{col}_rolling_std_7d'] = df.groupby(group_cols)[col].transform(\n",
    "        lambda x: x.rolling(7, min_periods=1).std()\n",
    "    )\n",
    "    df[f'{col}_rolling_mean_30d'] = df.groupby(group_cols)[col].transform(\n",
    "        lambda x: x.rolling(30, min_periods=1).mean()\n",
    "    )\n",
    "    lag_features_created += 8\n",
    "\n",
    "print(f\"✓ Created {lag_features_created} lag/rolling features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "655a3c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating spatial features...\n",
      "✓ Created 10 spatial features\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Spatial Features\n",
    "print(\"Creating spatial features...\")\n",
    "\n",
    "# State-level aggregates\n",
    "state_stats = df.groupby(['state', 'date']).agg({\n",
    "    'total_enrollments': ['sum', 'mean', 'std', 'count']\n",
    "}).reset_index()\n",
    "state_stats.columns = ['state', 'date', 'state_total', 'state_mean', 'state_std', 'state_count']\n",
    "df = df.merge(state_stats, on=['state', 'date'], how='left')\n",
    "\n",
    "# District-level aggregates\n",
    "district_stats = df.groupby(['state', 'district', 'date']).agg({\n",
    "    'total_enrollments': ['sum', 'mean', 'count']\n",
    "}).reset_index()\n",
    "district_stats.columns = ['state', 'district', 'date', 'district_total', 'district_mean', 'district_count']\n",
    "df = df.merge(district_stats, on=['state', 'district', 'date'], how='left')\n",
    "\n",
    "# Ratio features\n",
    "df['pincode_vs_state_ratio'] = df['total_enrollments'] / (df['state_mean'] + 1e-10)\n",
    "df['pincode_vs_district_ratio'] = df['total_enrollments'] / (df['district_mean'] + 1e-10)\n",
    "df['district_vs_state_ratio'] = df['district_mean'] / (df['state_mean'] + 1e-10)\n",
    "\n",
    "print(f\"✓ Created {10} spatial features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4dfe794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating anomaly features...\n",
      "✓ Created 4 anomaly detection features\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Anomaly Detection Features\n",
    "print(\"Creating anomaly features...\")\n",
    "df['z_score_state'] = (df['total_enrollments'] - df['state_mean']) / (df['state_std'] + 1e-10)\n",
    "df['z_score_rolling'] = (df['total_enrollments'] - df['total_enrollments_rolling_mean_7d']) / (df['total_enrollments_rolling_std_7d'] + 1e-10)\n",
    "df['enrollment_volatility'] = df['total_enrollments_rolling_std_7d'] / (df['total_enrollments_rolling_mean_7d'] + 1e-10)\n",
    "df['is_spike'] = (abs(df['z_score_rolling']) > 2).astype(int)\n",
    "\n",
    "print(f\"✓ Created {4} anomaly detection features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ba789d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating interaction features...\n",
      "✓ Created 1 interaction feature\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Interaction Features\n",
    "print(\"Creating interaction features...\")\n",
    "df['child_youth_interaction'] = df['age_0_5'] * df['age_5_17']\n",
    "\n",
    "print(f\"✓ Created {1} interaction feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "817b3477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FEATURE ENGINEERING COMPLETE\n",
      "============================================================\n",
      "Total shape: (219091, 77)\n",
      "Total features: 77\n",
      "\n",
      "NaN Summary:\n",
      "  - Lag features: 1,463,288 NaNs\n",
      "  - Rolling features: 90,292 NaNs\n",
      "  - Pct change: 983,398 NaNs\n",
      "  (AutoGluon will handle NaN imputation)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Feature Engineering Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total shape: {df.shape}\")\n",
    "print(f\"Total features: {len(df.columns)}\")\n",
    "\n",
    "print(\"\\nNaN Summary:\")\n",
    "print(f\"  - Lag features: {df[[c for c in df.columns if '_lag' in c]].isna().sum().sum():,} NaNs\")\n",
    "print(f\"  - Rolling features: {df[[c for c in df.columns if '_rolling_' in c]].isna().sum().sum():,} NaNs\")\n",
    "print(f\"  - Pct change: {df[[c for c in df.columns if '_pct_change_' in c]].isna().sum().sum():,} NaNs\")\n",
    "print(\"  (AutoGluon will handle NaN imputation)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4acd7f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING TARGET VARIABLES (LEAK-FREE)\n",
      "============================================================\n",
      "Train/Test cutoff date: 2025-09-25\n",
      "  Training samples: 179,423\n",
      "  Test samples: 39,668\n",
      "\n",
      "Task 1 - Anomaly Detection\n",
      "  Volatility threshold (training data only): 1.168708\n",
      "  Overall anomaly rate: 7.91%\n",
      "  Train anomaly rate: 7.83%\n",
      "  Test anomaly rate: 8.29%\n",
      "\n",
      "Task 2 - 7-Day Forecasting\n",
      "  Target variable: Predict total_enrollments 7 days ahead\n",
      "  NaN rows (last 7 days): 126,781\n",
      "\n",
      "Task 3 - Spatial Inequality\n",
      "  Z-score threshold (training data only): 1.1365\n",
      "  Overall inequality rate: 9.99%\n",
      "  Train inequality rate: 10.00%\n",
      "  Test inequality rate: 9.94%\n",
      "\n",
      "Dataset Sizes:\n",
      "  Task 1/3: 219,091 samples\n",
      "  Task 2:   92,310 samples (after dropping NaN targets)\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Target Variable Creation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING TARGET VARIABLES (LEAK-FREE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compute train/test cutoff BEFORE creating targets\n",
    "TRAIN_TEST_CUTOFF = df['date'].quantile(0.8)\n",
    "train_mask = df['date'] <= TRAIN_TEST_CUTOFF\n",
    "print(f\"Train/Test cutoff date: {TRAIN_TEST_CUTOFF.date()}\")\n",
    "print(f\"  Training samples: {train_mask.sum():,}\")\n",
    "print(f\"  Test samples: {(~train_mask).sum():,}\")\n",
    "\n",
    "# === Task 1: Anomaly Detection ===\n",
    "TARGET_TASK1 = 'is_anomaly'\n",
    "\n",
    "volatility_threshold = df.loc[train_mask, 'enrollment_volatility'].quantile(0.95)\n",
    "print(f\"\\nTask 1 - Anomaly Detection\")\n",
    "print(f\"  Volatility threshold (training data only): {volatility_threshold:.6f}\")\n",
    "\n",
    "df[TARGET_TASK1] = (\n",
    "    (abs(df['z_score_rolling']) > 2) |\n",
    "    (abs(df['z_score_state']) > 2.5) |\n",
    "    (df['enrollment_volatility'] > volatility_threshold)\n",
    ").astype(int)\n",
    "\n",
    "print(f\"  Overall anomaly rate: {df[TARGET_TASK1].mean():.2%}\")\n",
    "print(f\"  Train anomaly rate: {df.loc[train_mask, TARGET_TASK1].mean():.2%}\")\n",
    "print(f\"  Test anomaly rate: {df.loc[~train_mask, TARGET_TASK1].mean():.2%}\")\n",
    "\n",
    "# === Task 2: 7-Day Forecasting ===\n",
    "TARGET_TASK2 = 'target_7d'\n",
    "df[TARGET_TASK2] = df.groupby(['state', 'district', 'pincode'])['total_enrollments'].shift(-7)\n",
    "\n",
    "print(f\"\\nTask 2 - 7-Day Forecasting\")\n",
    "print(f\"  Target variable: Predict total_enrollments 7 days ahead\")\n",
    "print(f\"  NaN rows (last 7 days): {df[TARGET_TASK2].isna().sum():,}\")\n",
    "\n",
    "# === Task 3: Spatial Inequality ===\n",
    "TARGET_TASK3 = 'high_inequality'\n",
    "\n",
    "threshold_inequality = df.loc[train_mask, 'z_score_state'].quantile(0.90)\n",
    "df[TARGET_TASK3] = (df['z_score_state'] > threshold_inequality).astype(int)\n",
    "\n",
    "print(f\"\\nTask 3 - Spatial Inequality\")\n",
    "print(f\"  Z-score threshold (training data only): {threshold_inequality:.4f}\")\n",
    "print(f\"  Overall inequality rate: {df[TARGET_TASK3].mean():.2%}\")\n",
    "print(f\"  Train inequality rate: {df.loc[train_mask, TARGET_TASK3].mean():.2%}\")\n",
    "print(f\"  Test inequality rate: {df.loc[~train_mask, TARGET_TASK3].mean():.2%}\")\n",
    "\n",
    "# --- Handle NaNs from Target Creation ---\n",
    "df_task2 = df.dropna(subset=[TARGET_TASK2])\n",
    "df_task1 = df.copy()\n",
    "df_task3 = df.copy()\n",
    "\n",
    "df_task3 = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "print(f\"\\nDataset Sizes:\")\n",
    "print(f\"  Task 1/3: {df_task1.shape[0]:,} samples\")\n",
    "print(f\"  Task 2:   {df_task2.shape[0]:,} samples (after dropping NaN targets)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2ca7975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DEFINING FEATURE SETS (LEAK-FREE)\n",
      "============================================================\n",
      "\n",
      "Task 1 Features: 50\n",
      "  - Categorical: 3\n",
      "  - Excluded: 23 leaking features\n",
      "\n",
      "Task 2 Features: 51\n",
      "  - Categorical: 3\n",
      "  ✓ No duplicate columns\n",
      "\n",
      "Task 3 Features: 58\n",
      "  - Categorical: 3\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Define Feature Sets (Leak-Free)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEFINING FEATURE SETS (LEAK-FREE)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ALL_FEATURES = list(df.columns)\n",
    "ID_COLS = ['state', 'district', 'pincode', 'date']\n",
    "TARGET_COLS = [TARGET_TASK1, TARGET_TASK2, TARGET_TASK3]\n",
    "RAW_VALUE_COLS = ['age_0_5', 'age_5_17', 'age_18_greater']\n",
    "CATEGORICAL_COLS = ['state', 'district', 'pincode']\n",
    "\n",
    "FREQUENCY_COLS = [col for col in ALL_FEATURES if '_frequency' in col]\n",
    "\n",
    "# === Task 1: Anomaly Detection ===\n",
    "LEAKING_COLS_TASK1 = [\n",
    "    'z_score_rolling',\n",
    "    'is_spike',\n",
    "    'z_score_state',\n",
    "    'enrollment_volatility',\n",
    "    'state_total', 'state_mean', 'state_std', 'state_count',\n",
    "    'district_total', 'district_mean', 'district_count',\n",
    "    'pincode_vs_state_ratio',\n",
    "    'pincode_vs_district_ratio',\n",
    "    'district_vs_state_ratio',\n",
    "    'total_enrollments',\n",
    "]\n",
    "LEAKING_COLS_TASK1.extend([col for col in ALL_FEATURES if '_pct_change_' in col])\n",
    "\n",
    "FINAL_FEATURES_TASK1 = [\n",
    "    col for col in ALL_FEATURES \n",
    "    if col not in ID_COLS + TARGET_COLS + RAW_VALUE_COLS + LEAKING_COLS_TASK1 + FREQUENCY_COLS\n",
    "]\n",
    "FINAL_FEATURES_TASK1.extend(CATEGORICAL_COLS)\n",
    "\n",
    "print(f\"\\nTask 1 Features: {len(FINAL_FEATURES_TASK1)}\")\n",
    "print(f\"  - Categorical: {len(CATEGORICAL_COLS)}\")\n",
    "print(f\"  - Excluded: {len(LEAKING_COLS_TASK1)} leaking features\")\n",
    "\n",
    "# === Task 2: Forecasting (FIXED - No Duplicates) ===\n",
    "LEAKING_COLS_TASK2 = [\n",
    "    'state_total', 'state_mean', 'state_std', 'state_count',\n",
    "    'district_total', 'district_mean', 'district_count',\n",
    "    'pincode_vs_state_ratio', 'pincode_vs_district_ratio',\n",
    "    'district_vs_state_ratio',\n",
    "    'z_score_state', 'z_score_rolling', 'enrollment_volatility', 'is_spike'\n",
    "]\n",
    "LEAKING_COLS_TASK2.extend([col for col in ALL_FEATURES if '_pct_change_' in col])\n",
    "\n",
    "FINAL_FEATURES_TASK2 = [\n",
    "    col for col in ALL_FEATURES \n",
    "    if col not in ID_COLS + TARGET_COLS + RAW_VALUE_COLS + LEAKING_COLS_TASK2 + FREQUENCY_COLS\n",
    "]\n",
    "FINAL_FEATURES_TASK2.extend(CATEGORICAL_COLS)\n",
    "\n",
    "# FIX: Only add total_enrollments if not already present\n",
    "if 'total_enrollments' not in FINAL_FEATURES_TASK2:\n",
    "    FINAL_FEATURES_TASK2.append('total_enrollments')\n",
    "\n",
    "# Verify no duplicates\n",
    "assert len(FINAL_FEATURES_TASK2) == len(set(FINAL_FEATURES_TASK2)), \"Task 2 has duplicate features!\"\n",
    "\n",
    "print(f\"\\nTask 2 Features: {len(FINAL_FEATURES_TASK2)}\")\n",
    "print(f\"  - Categorical: {len(CATEGORICAL_COLS)}\")\n",
    "print(f\"  ✓ No duplicate columns\")\n",
    "\n",
    "# === Task 3: Spatial Inequality ===\n",
    "LEAKING_COLS_TASK3 = [\n",
    "    'state_total', 'state_mean', 'state_std', 'state_count',\n",
    "    'district_total', 'district_mean', 'district_count',\n",
    "    'z_score_state',\n",
    "    'pincode_vs_state_ratio',\n",
    "    'pincode_vs_district_ratio',\n",
    "    'district_vs_state_ratio',\n",
    "    'z_score_rolling', 'is_spike', 'enrollment_volatility',\n",
    "    'total_enrollments'\n",
    "]\n",
    "\n",
    "FINAL_FEATURES_TASK3 = [\n",
    "    col for col in ALL_FEATURES \n",
    "    if col not in ID_COLS + TARGET_COLS + RAW_VALUE_COLS + LEAKING_COLS_TASK3 + FREQUENCY_COLS\n",
    "]\n",
    "FINAL_FEATURES_TASK3.extend(CATEGORICAL_COLS)\n",
    "\n",
    "print(f\"\\nTask 3 Features: {len(FINAL_FEATURES_TASK3)}\")\n",
    "print(f\"  - Categorical: {len(CATEGORICAL_COLS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e8e62a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ CV function defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Define CV Function\n",
    "def run_time_series_cv(train_data, df_dates, task_name, label, eval_metric, problem_type, n_splits=3):\n",
    "    \"\"\"\n",
    "    Perform time-series cross-validation with detailed reporting\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TIME-SERIES CV: {task_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(train_data)):\n",
    "        print(f\"\\nFold {fold_idx + 1}/{n_splits}\")\n",
    "        \n",
    "        X_fold_train = train_data.iloc[train_idx]\n",
    "        X_fold_val = train_data.iloc[val_idx]\n",
    "        \n",
    "        train_dates = df_dates.iloc[train_idx]\n",
    "        val_dates = df_dates.iloc[val_idx]\n",
    "        \n",
    "        print(f\"  Train: {len(X_fold_train):,} samples | {train_dates.min().date()} to {train_dates.max().date()}\")\n",
    "        print(f\"  Val:   {len(X_fold_val):,} samples | {val_dates.min().date()} to {val_dates.max().date()}\")\n",
    "        \n",
    "        predictor_cv = TabularPredictor(\n",
    "            label=label,\n",
    "            path=f\"autogluon_models/{task_name}_cv_fold{fold_idx}\",\n",
    "            eval_metric=eval_metric,\n",
    "            problem_type=problem_type,\n",
    "            verbosity=0\n",
    "        )\n",
    "        \n",
    "        predictor_cv.fit(\n",
    "            X_fold_train,\n",
    "            presets='medium_quality',\n",
    "            time_limit=600,\n",
    "            ag_args_fit={'num_gpus': 0}\n",
    "        )\n",
    "        \n",
    "        # Get the score - evaluate returns a dict, extract the metric value\n",
    "        score_dict = predictor_cv.evaluate(X_fold_val, silent=True)\n",
    "        if isinstance(score_dict, dict):\n",
    "            score = score_dict.get(eval_metric, list(score_dict.values())[0])\n",
    "        else:\n",
    "            score = score_dict\n",
    "        cv_scores.append(score)\n",
    "        \n",
    "        print(f\"  Fold {fold_idx + 1} {eval_metric}: {score:.4f}\")\n",
    "        \n",
    "        predictor_cv.save_space()\n",
    "    \n",
    "    print(f\"\\n{task_name} CV Results:\")\n",
    "    print(f\"  Mean {eval_metric}: {np.mean(cv_scores):.4f} +/- {np.std(cv_scores):.4f}\")\n",
    "    print(f\"  Min: {np.min(cv_scores):.4f} | Max: {np.max(cv_scores):.4f}\")\n",
    "    \n",
    "    return cv_scores\n",
    "\n",
    "print(\"✓ CV function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e4e959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TASK 1: ANOMALY DETECTION\n",
      "============================================================\n",
      "\n",
      "Pre-split class distribution:\n",
      "is_anomaly\n",
      "0    201756\n",
      "1     17335\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time-based split:\n",
      "  Train: 179,423 samples (up to 2025-09-25)\n",
      "  Test:  39,668 samples (after 2025-09-25)\n",
      "  Train anomaly rate: 7.83%\n",
      "  Test anomaly rate:  8.29%\n",
      "\n",
      "============================================================\n",
      "TIME-SERIES CV: task1_anomaly\n",
      "============================================================\n",
      "\n",
      "Fold 1/3\n",
      "  Train: 44,858 samples | 2025-03-09 to 2025-09-25\n",
      "  Val:   44,855 samples | 2025-03-09 to 2025-09-25\n",
      "  Fold 1 roc_auc: 0.9734\n",
      "\n",
      "Fold 2/3\n",
      "  Train: 89,713 samples | 2025-03-09 to 2025-09-25\n",
      "  Val:   44,855 samples | 2025-03-02 to 2025-09-25\n",
      "  Fold 2 roc_auc: 0.9835\n",
      "\n",
      "Fold 3/3\n",
      "  Train: 134,568 samples | 2025-03-02 to 2025-09-25\n",
      "  Val:   44,855 samples | 2025-03-09 to 2025-09-25\n",
      "  Fold 3 roc_auc: 0.9891\n",
      "\n",
      "task1_anomaly CV Results:\n",
      "  Mean roc_auc: 0.9820 +/- 0.0065\n",
      "  Min: 0.9734 | Max: 0.9891\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Task 1 - Anomaly Detection (CV + Training)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 1: ANOMALY DETECTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_data_task1 = df_task1[FINAL_FEATURES_TASK1 + [TARGET_TASK1]].copy()\n",
    "\n",
    "for cat_col in CATEGORICAL_COLS:\n",
    "    train_data_task1[cat_col] = train_data_task1[cat_col].astype('category')\n",
    "\n",
    "print(f\"\\nPre-split class distribution:\")\n",
    "print(train_data_task1[TARGET_TASK1].value_counts())\n",
    "if train_data_task1[TARGET_TASK1].nunique() < 2:\n",
    "    raise ValueError(\"Only one class present in Task 1!\")\n",
    "\n",
    "X_train_t1 = train_data_task1[df_task1['date'] <= TRAIN_TEST_CUTOFF]\n",
    "X_test_t1 = train_data_task1[df_task1['date'] > TRAIN_TEST_CUTOFF]\n",
    "\n",
    "print(f\"\\nTime-based split:\")\n",
    "print(f\"  Train: {len(X_train_t1):,} samples (up to {TRAIN_TEST_CUTOFF.date()})\")\n",
    "print(f\"  Test:  {len(X_test_t1):,} samples (after {TRAIN_TEST_CUTOFF.date()})\")\n",
    "print(f\"  Train anomaly rate: {X_train_t1[TARGET_TASK1].mean():.2%}\")\n",
    "print(f\"  Test anomaly rate:  {X_test_t1[TARGET_TASK1].mean():.2%}\")\n",
    "\n",
    "# Run CV\n",
    "cv_scores_t1 = run_time_series_cv(\n",
    "    X_train_t1,\n",
    "    df_task1.loc[X_train_t1.index, 'date'],\n",
    "    'task1_anomaly',\n",
    "    TARGET_TASK1,\n",
    "    'roc_auc',\n",
    "    'binary',\n",
    "    n_splits=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c01ce445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"autogluon_models/task1_anomaly_PRODUCTION\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Oct 23 15:35:13 UTC 2025\n",
      "CPU Count:          12\n",
      "Memory Avail:       5.99 GB / 15.25 GB (39.3%)\n",
      "Disk Space Avail:   423.65 GB / 464.17 GB (91.3%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 450s of the 1800s of remaining time (25%).\n",
      "2025-11-02 20:56:28,097\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final Task 1 model (30 min limit)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-02 20:56:29,515\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\t\tContext path: \"/home/vulcan/Abhay/Projects/ADA/Baseline-2/autogluon_models/task1_anomaly_PRODUCTION/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Running DyStack sub-fit ...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Beginning AutoGluon training ... Time limit = 448s\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m AutoGluon will save models to \"/home/vulcan/Abhay/Projects/ADA/Baseline-2/autogluon_models/task1_anomaly_PRODUCTION/ds_sub_fit/sub_fit_ho\"\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Train Data Rows:    159487\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Train Data Columns: 50\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Label Column:       is_anomaly\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Problem Type:       binary\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Preprocessing data ...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Using Feature Generators to preprocess the data ...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tAvailable Memory:                    5608.03 MB\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tTrain Data (Original)  Memory Usage: 54.49 MB (1.0% of available memory)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tStage 1 Generators:\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tStage 2 Generators:\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tStage 3 Generators:\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tStage 4 Generators:\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tStage 5 Generators:\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tUseless Original Features (Count: 2): ['year', 'is_month_end']\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\tThis is typically a feature which has the same value for all rows.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\tThese features do not need to be present at inference time.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\t('category', []) :  3 | ['state', 'district', 'pincode']\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\t('float', [])    : 36 | ['day_sin', 'day_cos', 'month_sin', 'month_cos', 'dow_sin', ...]\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\t('int', [])      :  9 | ['month', 'day', 'day_of_week', 'day_of_year', 'week_of_year', ...]\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\t('category', [])  :  3 | ['state', 'district', 'pincode']\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\t('float', [])     : 36 | ['day_sin', 'day_cos', 'month_sin', 'month_cos', 'dow_sin', ...]\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\t('int', [])       :  6 | ['month', 'day', 'day_of_week', 'day_of_year', 'week_of_year', ...]\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t\t('int', ['bool']) :  3 | ['is_weekend', 'is_month_start', 'days_in_month']\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.9s = Fit runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t48 features in original data used to generate 48 features in processed data.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tTrain Data (Processed) Memory Usage: 49.89 MB (0.9% of available memory)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Data preprocessing and feature engineering runtime = 1.15s ...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m User-specified model hyperparameters to be fit:\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m {\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m }\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 297.74s of the 446.72s of remaining time.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.19%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=126238)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.0456411\n",
      "\u001b[36m(_ray_fit pid=126241)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.0490498\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=126239)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.0476152\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=126238)\u001b[0m [2000]\tvalid_set's binary_logloss: 0.0435471\n",
      "\u001b[36m(_ray_fit pid=126242)\u001b[0m [2000]\tvalid_set's binary_logloss: 0.0452567\n",
      "\u001b[36m(_ray_fit pid=126236)\u001b[0m [2000]\tvalid_set's binary_logloss: 0.042709\n",
      "\u001b[36m(_ray_fit pid=126240)\u001b[0m [2000]\tvalid_set's binary_logloss: 0.0436297\n",
      "\u001b[36m(_ray_fit pid=126239)\u001b[0m [2000]\tvalid_set's binary_logloss: 0.0457564\u001b[32m [repeated 4x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.9945\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t90.77s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t58.22s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 198.50s of the 347.48s of remaining time.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.29%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_ray_fit pid=126972)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.0472208\n",
      "\u001b[36m(_ray_fit pid=126974)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.0483599\n",
      "\u001b[36m(_ray_fit pid=126971)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.0460921\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=126972)\u001b[0m [2000]\tvalid_set's binary_logloss: 0.0465112\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=126971)\u001b[0m [2000]\tvalid_set's binary_logloss: 0.0451534\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=126972)\u001b[0m [3000]\tvalid_set's binary_logloss: 0.0460778\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(_ray_fit pid=126971)\u001b[0m [3000]\tvalid_set's binary_logloss: 0.0458322\n",
      "\u001b[36m(_ray_fit pid=126972)\u001b[0m [4000]\tvalid_set's binary_logloss: 0.047536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.9943\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t91.67s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t39.44s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 98.87s of the 247.85s of remaining time.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.9926\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t17.06s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t3.5s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 78.05s of the 227.03s of remaining time.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.9931\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t13.22s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t3.37s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 61.24s of the 210.22s of remaining time.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 16.11% memory usage per fold, 64.46%/80.00% total).\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=16.11%)\n",
      "\u001b[36m(_ray_fit pid=127794)\u001b[0m \tRan out of time, early stopping on iteration 164.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.9882\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t49.21s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.39s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 10.35s of the 159.32s of remaining time.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 73 due to low time. Expected time usage reduced from 41.3s -> 10.2s...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.9803\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t3.82s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t1.08s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 5.24s of the 154.22s of remaining time.\n",
      "\u001b[36m(_ray_fit pid=127795)\u001b[0m \tRan out of time, early stopping on iteration 166.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 91 due to low time. Expected time usage reduced from 16.6s -> 5.1s...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.9817\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t3.87s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t1.33s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 148.81s of remaining time.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tEnsemble Weights: {'LightGBM_BAG_L1': 0.609, 'LightGBMXT_BAG_L1': 0.261, 'RandomForestEntr_BAG_L1': 0.087, 'RandomForestGini_BAG_L1': 0.043}\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.9961\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t2.85s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.02s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 145.93s of the 145.90s of remaining time.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.58%)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.9965\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t11.04s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.97s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 132.98s of the 132.96s of remaining time.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.82%)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.9956\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t7.22s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.55s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 124.06s of the 124.03s of remaining time.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.9959\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t20.02s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t3.13s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 100.73s of the 100.71s of remaining time.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.9961\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t12.51s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t2.95s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 85.11s of the 85.09s of remaining time.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 14.02% memory usage per fold, 56.09%/80.00% total).\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=14.02%)\n",
      "\u001b[36m(_ray_fit pid=129939)\u001b[0m \tRan out of time, early stopping on iteration 242.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.9967\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t68.46s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.5s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 15.06s of the 15.04s of remaining time.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 269 due to low time. Expected time usage reduced from 16.5s -> 14.8s...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tNot enough time to generate out-of-fold predictions for model. Estimated time required was 10.75s compared to 10.05s of available time.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tTime limit exceeded... Skipping ExtraTreesGini_BAG_L2.\n",
      "\u001b[36m(_ray_fit pid=129943)\u001b[0m \tRan out of time, early stopping on iteration 268.\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 5.39s of the 5.36s of remaining time.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 158 due to low time. Expected time usage reduced from 9.9s -> 5.2s...\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.9945\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t5.08s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t1.85s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -1.78s of remaining time.\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \tEnsemble Weights: {'RandomForestGini_BAG_L2': 0.294, 'RandomForestEntr_BAG_L2': 0.294, 'CatBoost_BAG_L2': 0.235, 'LightGBMXT_BAG_L2': 0.118, 'ExtraTreesEntr_BAG_L2': 0.059}\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.9971\t = Validation score   (roc_auc)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t5.28s\t = Training   runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m \t0.02s\t = Validation runtime\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m AutoGluon training complete, total runtime = 455.02s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 196.1 rows/s (19936 batch size)\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/vulcan/Abhay/Projects/ADA/Baseline-2/autogluon_models/task1_anomaly_PRODUCTION/ds_sub_fit/sub_fit_ho\")\n",
      "\u001b[36m(_dystack pid=125455)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                      model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0       WeightedEnsemble_L3       0.997507   0.997133     roc_auc       11.238811     116.739235  392.003377                 0.002210                0.016479           5.281957            3       True         15\n",
      "1   RandomForestEntr_BAG_L2       0.997276   0.996082     roc_auc       10.619616     110.279827  282.120465                 0.114464                2.946004          12.506276            2       True         12\n",
      "2           CatBoost_BAG_L2       0.996892   0.996685     roc_auc       10.675486     107.832404  338.072524                 0.170334                0.498581          68.458335            2       True         13\n",
      "3         LightGBMXT_BAG_L2       0.996795   0.996496     roc_auc       10.723342     108.299016  280.658770                 0.218190                0.965193          11.044581            2       True          9\n",
      "4       WeightedEnsemble_L2       0.996412   0.996082     roc_auc        9.786201     104.543052  215.567829                 0.002537                0.016612           2.850344            2       True          8\n",
      "5   RandomForestGini_BAG_L2       0.996346   0.995930     roc_auc       10.662015     110.466985  289.629847                 0.156863                3.133162          20.015658            2       True         11\n",
      "6     ExtraTreesEntr_BAG_L2       0.996256   0.994506     roc_auc       10.576751     109.179815  274.696571                 0.071599                1.845992           5.082382            2       True         14\n",
      "7           LightGBM_BAG_L2       0.996170   0.995554     roc_auc       10.632756     107.882906  276.836711                 0.127604                0.549083           7.222522            2       True         10\n",
      "8           LightGBM_BAG_L1       0.995878   0.994311     roc_auc        4.930665      39.435134   91.672818                 4.930665               39.435134          91.672818            1       True          2\n",
      "9         LightGBMXT_BAG_L1       0.994293   0.994452     roc_auc        4.236144      58.222804   90.768070                 4.236144               58.222804          90.768070            1       True          1\n",
      "10  RandomForestEntr_BAG_L1       0.993706   0.993129     roc_auc        0.290030       3.366165   13.216153                 0.290030                3.366165          13.216153            1       True          4\n",
      "11  RandomForestGini_BAG_L1       0.993601   0.992641     roc_auc        0.326824       3.502338   17.060444                 0.326824                3.502338          17.060444            1       True          3\n",
      "12          CatBoost_BAG_L1       0.988680   0.988164     roc_auc        0.358893       0.392500   49.205820                 0.358893                0.392500          49.205820            1       True          5\n",
      "13    ExtraTreesGini_BAG_L1       0.984093   0.980309     roc_auc        0.163680       1.082459    3.824759                 0.163680                1.082459           3.824759            1       True          6\n",
      "14    ExtraTreesEntr_BAG_L1       0.983449   0.981726     roc_auc        0.198915       1.332424    3.866125                 0.198915                1.332424           3.866125            1       True          7\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t470s\t = DyStack   runtime |\t1330s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 1330s\n",
      "AutoGluon will save models to \"/home/vulcan/Abhay/Projects/ADA/Baseline-2/autogluon_models/task1_anomaly_PRODUCTION\"\n",
      "Train Data Rows:    179423\n",
      "Train Data Columns: 50\n",
      "Label Column:       is_anomaly\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5789.28 MB\n",
      "\tTrain Data (Original)  Memory Usage: 61.27 MB (1.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 2): ['year', 'is_month_end']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['state', 'district', 'pincode']\n",
      "\t\t('float', [])    : 36 | ['day_sin', 'day_cos', 'month_sin', 'month_cos', 'dow_sin', ...]\n",
      "\t\t('int', [])      :  9 | ['month', 'day', 'day_of_week', 'day_of_year', 'week_of_year', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  3 | ['state', 'district', 'pincode']\n",
      "\t\t('float', [])     : 36 | ['day_sin', 'day_cos', 'month_sin', 'month_cos', 'dow_sin', ...]\n",
      "\t\t('int', [])       :  6 | ['month', 'day', 'day_of_week', 'day_of_year', 'week_of_year', ...]\n",
      "\t\t('int', ['bool']) :  3 | ['is_weekend', 'is_month_start', 'days_in_month']\n",
      "\t0.4s = Fit runtime\n",
      "\t48 features in original data used to generate 48 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 56.13 MB (0.9% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 885.84s of the 1329.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.06%)\n",
      "\t0.9945\t = Validation score   (roc_auc)\n",
      "\t92.05s\t = Training   runtime\n",
      "\t89.5s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 784.57s of the 1227.82s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.08%)\n",
      "\t0.9952\t = Validation score   (roc_auc)\n",
      "\t117.58s\t = Training   runtime\n",
      "\t46.29s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 656.64s of the 1099.88s of remaining time.\n",
      "\t0.993\t = Validation score   (roc_auc)\n",
      "\t16.39s\t = Training   runtime\n",
      "\t3.74s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 636.12s of the 1079.37s of remaining time.\n",
      "\t0.9935\t = Validation score   (roc_auc)\n",
      "\t13.96s\t = Training   runtime\n",
      "\t3.57s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 617.93s of the 1061.18s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.81% memory usage per fold, 47.23%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=11.81%)\n",
      "\t0.9962\t = Validation score   (roc_auc)\n",
      "\t496.24s\t = Training   runtime\n",
      "\t1.21s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 120.10s of the 563.35s of remaining time.\n",
      "\t0.9869\t = Validation score   (roc_auc)\n",
      "\t13.34s\t = Training   runtime\n",
      "\t4.47s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 101.55s of the 544.80s of remaining time.\n",
      "\t0.9866\t = Validation score   (roc_auc)\n",
      "\t12.26s\t = Training   runtime\n",
      "\t4.44s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 84.16s of the 527.41s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.55%)\n",
      "\t0.9653\t = Validation score   (roc_auc)\n",
      "\t67.1s\t = Training   runtime\n",
      "\t1.96s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 15.44s of the 458.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.26%)\n",
      "\t0.9932\t = Validation score   (roc_auc)\n",
      "\t12.69s\t = Training   runtime\n",
      "\t0.83s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 0.82s of the 444.07s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.58%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 435.17s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 0.522, 'LightGBM_BAG_L1': 0.261, 'LightGBMXT_BAG_L1': 0.087, 'RandomForestEntr_BAG_L1': 0.087, 'RandomForestGini_BAG_L1': 0.043}\n",
      "\t0.9967\t = Validation score   (roc_auc)\n",
      "\t4.14s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 430.98s of the 430.94s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.76%)\n",
      "2025-11-02 21:19:18,063\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-11-02 21:19:18,066\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-11-02 21:19:18,067\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-11-02 21:19:18,068\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-11-02 21:19:18,068\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-11-02 21:19:18,069\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-11-02 21:19:18,069\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t0.9971\t = Validation score   (roc_auc)\n",
      "\t12.46s\t = Training   runtime\n",
      "\t1.36s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 416.79s of the 416.75s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.72%)\n",
      "\t0.9958\t = Validation score   (roc_auc)\n",
      "\t8.11s\t = Training   runtime\n",
      "\t0.53s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 407.18s of the 407.14s of remaining time.\n",
      "\t0.9962\t = Validation score   (roc_auc)\n",
      "\t22.8s\t = Training   runtime\n",
      "\t3.36s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 380.78s of the 380.74s of remaining time.\n",
      "\t0.9964\t = Validation score   (roc_auc)\n",
      "\t14.83s\t = Training   runtime\n",
      "\t3.17s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 362.57s of the 362.53s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.42% memory usage per fold, 49.69%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=12.42%)\n",
      "\t0.9978\t = Validation score   (roc_auc)\n",
      "\t290.56s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 70.45s of the 70.41s of remaining time.\n",
      "\t0.9963\t = Validation score   (roc_auc)\n",
      "\t11.44s\t = Training   runtime\n",
      "\t3.84s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 54.83s of the 54.79s of remaining time.\n",
      "\t0.9964\t = Validation score   (roc_auc)\n",
      "\t10.32s\t = Training   runtime\n",
      "\t3.67s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 40.54s of the 40.50s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.49%)\n",
      "\t0.9953\t = Validation score   (roc_auc)\n",
      "\t31.73s\t = Training   runtime\n",
      "\t1.97s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 7.00s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L2': 0.667, 'ExtraTreesGini_BAG_L2': 0.125, 'RandomForestEntr_BAG_L2': 0.083, 'ExtraTreesEntr_BAG_L2': 0.083, 'RandomForestGini_BAG_L2': 0.042}\n",
      "\t0.9979\t = Validation score   (roc_auc)\n",
      "\t7.95s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1330.72s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 155.5 rows/s (22428 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/vulcan/Abhay/Projects/ADA/Baseline-2/autogluon_models/task1_anomaly_PRODUCTION\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task 1 Final Results:\n",
      "\n",
      "Leaderboard (Top 5 models):\n",
      "                 model  score_test  score_val  pred_time_test     fit_time\n",
      "0    LightGBMXT_BAG_L2    0.968810   0.997069       35.352586   854.072739\n",
      "1  WeightedEnsemble_L2    0.967313   0.996736       30.283732   740.356148\n",
      "2      CatBoost_BAG_L2    0.967146   0.997796       35.228734  1132.165778\n",
      "3      LightGBM_BAG_L2    0.965759   0.995754       35.039743   849.718428\n",
      "4      CatBoost_BAG_L1    0.965758   0.996169        0.686730   496.237592\n",
      "\n",
      "Test ROC-AUC: 0.9652\n",
      "CV ROC-AUC: 0.9820 +/- 0.0065\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Task 1 - Final Model Training\n",
    "predictor_task1 = TabularPredictor(\n",
    "    label=TARGET_TASK1,\n",
    "    path=\"autogluon_models/task1_anomaly_PRODUCTION\",\n",
    "    eval_metric='roc_auc',\n",
    "    problem_type='binary'\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining final Task 1 model (30 min limit)...\")\n",
    "predictor_task1.fit(\n",
    "    X_train_t1,\n",
    "    presets='best_quality',\n",
    "    time_limit=1800,\n",
    "    ag_args_fit={'num_gpus': 0}\n",
    ")\n",
    "\n",
    "print(f\"\\nTask 1 Final Results:\")\n",
    "leaderboard_t1 = predictor_task1.leaderboard(X_test_t1, silent=True)\n",
    "print(\"\\nLeaderboard (Top 5 models):\")\n",
    "print(leaderboard_t1[['model', 'score_test', 'score_val', 'pred_time_test', 'fit_time']].head())\n",
    "\n",
    "eval_t1_dict = predictor_task1.evaluate(X_test_t1, silent=True)\n",
    "eval_t1 = eval_t1_dict.get('roc_auc', list(eval_t1_dict.values())[0]) if isinstance(eval_t1_dict, dict) else eval_t1_dict\n",
    "print(f\"\\nTest ROC-AUC: {eval_t1:.4f}\")\n",
    "print(f\"CV ROC-AUC: {np.mean(cv_scores_t1):.4f} +/- {np.std(cv_scores_t1):.4f}\")\n",
    "\n",
    "predictor_task1.save_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd5df331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TASK 2: 7-DAY FORECASTING\n",
      "============================================================\n",
      "Task 2 features: 51 (deduplicated)\n",
      "\n",
      "Time-based split:\n",
      "  Train: 74,339 samples (up to 2025-09-15)\n",
      "  Test:  17,971 samples (after 2025-09-15)\n",
      "✓ No duplicate columns in training data\n",
      "\n",
      "============================================================\n",
      "TIME-SERIES CV: task2_forecasting\n",
      "============================================================\n",
      "\n",
      "Fold 1/3\n",
      "  Train: 18,587 samples | 2025-03-09 to 2025-09-15\n",
      "  Val:   18,584 samples | 2025-03-09 to 2025-09-15\n",
      "  Fold 1 root_mean_squared_error: -4.7478\n",
      "\n",
      "Fold 2/3\n",
      "  Train: 37,171 samples | 2025-03-09 to 2025-09-15\n",
      "  Val:   18,584 samples | 2025-03-02 to 2025-09-15\n",
      "  Fold 2 root_mean_squared_error: -25.0665\n",
      "\n",
      "Fold 3/3\n",
      "  Train: 55,755 samples | 2025-03-02 to 2025-09-15\n",
      "  Val:   18,584 samples | 2025-03-09 to 2025-09-15\n",
      "  Fold 3 root_mean_squared_error: -4.4301\n",
      "\n",
      "task2_forecasting CV Results:\n",
      "  Mean root_mean_squared_error: -11.4148 +/- 9.6541\n",
      "  Min: -25.0665 | Max: -4.4301\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Task 2 - 7-Day Forecasting (CV + Training) - FIXED\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 2: 7-DAY FORECASTING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# FIX: Remove any potential duplicates in feature list before creating training data\n",
    "FINAL_FEATURES_TASK2_UNIQUE = list(dict.fromkeys(FINAL_FEATURES_TASK2))  # Preserves order, removes dupes\n",
    "print(f\"Task 2 features: {len(FINAL_FEATURES_TASK2_UNIQUE)} (deduplicated)\")\n",
    "\n",
    "train_data_task2 = df_task2[FINAL_FEATURES_TASK2_UNIQUE + [TARGET_TASK2]].copy()\n",
    "\n",
    "for cat_col in CATEGORICAL_COLS:\n",
    "    if cat_col in train_data_task2.columns:\n",
    "        train_data_task2[cat_col] = train_data_task2[cat_col].astype('category')\n",
    "\n",
    "cutoff_date_t2 = df_task2['date'].quantile(0.8)\n",
    "X_train_t2 = train_data_task2[df_task2['date'] <= cutoff_date_t2]\n",
    "X_test_t2 = train_data_task2[df_task2['date'] > cutoff_date_t2]\n",
    "\n",
    "print(f\"\\nTime-based split:\")\n",
    "print(f\"  Train: {len(X_train_t2):,} samples (up to {cutoff_date_t2.date()})\")\n",
    "print(f\"  Test:  {len(X_test_t2):,} samples (after {cutoff_date_t2.date()})\")\n",
    "\n",
    "# Verify no duplicate columns\n",
    "assert X_train_t2.columns.duplicated().sum() == 0, \"X_train_t2 has duplicate columns!\"\n",
    "print(\"✓ No duplicate columns in training data\")\n",
    "\n",
    "# Run CV\n",
    "cv_scores_t2 = run_time_series_cv(\n",
    "    X_train_t2,\n",
    "    df_task2.loc[X_train_t2.index, 'date'],\n",
    "    'task2_forecasting',\n",
    "    TARGET_TASK2,\n",
    "    'root_mean_squared_error',\n",
    "    'regression',\n",
    "    n_splits=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22843dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"autogluon_models/task2_forecasting_PRODUCTION\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Oct 23 15:35:13 UTC 2025\n",
      "CPU Count:          12\n",
      "Memory Avail:       3.90 GB / 15.25 GB (25.6%)\n",
      "Disk Space Avail:   423.06 GB / 464.17 GB (91.1%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 450s of the 1800s of remaining time (25%).\n",
      "\t\tContext path: \"/home/vulcan/Abhay/Projects/ADA/Baseline-2/autogluon_models/task2_forecasting_PRODUCTION/ds_sub_fit/sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 2 Target Diagnostics:\n",
      "Target column: target_7d\n",
      "Data type: float64\n",
      "Unique values: 88\n",
      "Min: 1.0\n",
      "Max: 2538.0\n",
      "Mean: 4.24\n",
      "\n",
      "First 10 values:\n",
      "0     1.0\n",
      "1     1.0\n",
      "2     1.0\n",
      "3     1.0\n",
      "12    1.0\n",
      "13    1.0\n",
      "14    1.0\n",
      "15    1.0\n",
      "16    1.0\n",
      "17    1.0\n",
      "Name: target_7d, dtype: float64\n",
      "\n",
      "Training final Task 2 model (30 min limit)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L3      -4.015254 -11.093464  root_mean_squared_error        5.446167      14.923217  350.780691                 0.001598                0.001001           0.054681            3       True         12\n",
      "1          CatBoost_BAG_L1      -4.042099 -13.886574  root_mean_squared_error        0.412622       0.282004  166.524093                 0.412622                0.282004         166.524093            1       True          4\n",
      "2          CatBoost_BAG_L2      -4.054214 -13.621659  root_mean_squared_error        4.384526       9.926616  299.874202                 0.075618                0.174602          24.109327            2       True         11\n",
      "3   RandomForestMSE_BAG_L2      -4.066184 -12.049713  root_mean_squared_error        4.558874      10.881897  317.201074                 0.249966                1.129883          41.436198            2       True         10\n",
      "4      WeightedEnsemble_L2      -4.132970 -11.399237  root_mean_squared_error        1.745145       2.645835   25.021694                 0.001714                0.000757           0.028838            2       True          7\n",
      "5        LightGBMXT_BAG_L2      -4.133310 -11.487326  root_mean_squared_error        5.194604      13.792333  309.289812                 0.885695                4.040319          33.524936            2       True          8\n",
      "6          LightGBM_BAG_L2      -4.148509 -12.277247  root_mean_squared_error        4.824984      12.439070  305.050848                 0.516076                2.687055          29.285972            2       True          9\n",
      "7           XGBoost_BAG_L1      -4.161541 -12.358641  root_mean_squared_error        0.376002       0.446588    9.723408                 0.376002                0.446588           9.723408            1       True          6\n",
      "8     ExtraTreesMSE_BAG_L1      -4.167685 -13.167247  root_mean_squared_error        0.838444       1.936651   16.087630                 0.838444                1.936651          16.087630            1       True          5\n",
      "9        LightGBMXT_BAG_L1      -4.186350 -11.415358  root_mean_squared_error        1.367428       2.198490   15.269448                 1.367428                2.198490          15.269448            1       True          1\n",
      "10         LightGBM_BAG_L1      -4.228536 -12.090923  root_mean_squared_error        0.606723       2.740419   24.589646                 0.606723                2.740419          24.589646            1       True          2\n",
      "11  RandomForestMSE_BAG_L1      -4.255099 -15.684684  root_mean_squared_error        0.707689       2.147861   43.570652                 0.707689                2.147861          43.570652            1       True          3\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t454s\t = DyStack   runtime |\t1346s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 1346s\n",
      "AutoGluon will save models to \"/home/vulcan/Abhay/Projects/ADA/Baseline-2/autogluon_models/task2_forecasting_PRODUCTION\"\n",
      "Train Data Rows:    74339\n",
      "Train Data Columns: 51\n",
      "Label Column:       target_7d\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    5620.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 26.00 MB (0.5% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 2): ['year', 'is_month_end']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tUnused Original Features (Count: 3): ['age_5_17_lag30', 'age_18_greater_lag30', 'total_enrollments_lag30']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('float', []) : 3 | ['age_5_17_lag30', 'age_18_greater_lag30', 'total_enrollments_lag30']\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['state', 'district', 'pincode']\n",
      "\t\t('float', [])    : 33 | ['day_sin', 'day_cos', 'month_sin', 'month_cos', 'dow_sin', ...]\n",
      "\t\t('int', [])      : 10 | ['month', 'day', 'day_of_week', 'day_of_year', 'week_of_year', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  3 | ['state', 'district', 'pincode']\n",
      "\t\t('float', [])     : 33 | ['day_sin', 'day_cos', 'month_sin', 'month_cos', 'dow_sin', ...]\n",
      "\t\t('int', [])       :  7 | ['month', 'day', 'day_of_week', 'day_of_year', 'week_of_year', ...]\n",
      "\t\t('int', ['bool']) :  3 | ['is_weekend', 'is_month_start', 'days_in_month']\n",
      "\t0.4s = Fit runtime\n",
      "\t46 features in original data used to generate 46 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 22.12 MB (0.4% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.45s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 106 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 896.90s of the 1345.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.49%)\n",
      "\t-10.8532\t = Validation score   (-root_mean_squared_error)\n",
      "\t20.83s\t = Training   runtime\n",
      "\t2.99s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 872.61s of the 1321.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.75%)\n",
      "\t-11.5125\t = Validation score   (-root_mean_squared_error)\n",
      "\t30.28s\t = Training   runtime\n",
      "\t3.52s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 838.25s of the 1287.03s of remaining time.\n",
      "\t-14.8863\t = Validation score   (-root_mean_squared_error)\n",
      "\t48.74s\t = Training   runtime\n",
      "\t1.93s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 786.96s of the 1235.74s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.74% memory usage per fold, 50.95%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=12.74%)\n",
      "\t-12.81\t = Validation score   (-root_mean_squared_error)\n",
      "\t175.24s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 610.35s of the 1059.13s of remaining time.\n",
      "\t-12.3704\t = Validation score   (-root_mean_squared_error)\n",
      "\t17.1s\t = Training   runtime\n",
      "\t1.91s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 590.89s of the 1039.67s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.04%)\n",
      "\t-11.2553\t = Validation score   (-root_mean_squared_error)\n",
      "\t355.33s\t = Training   runtime\n",
      "\t1.85s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 233.91s of the 682.69s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.58%)\n",
      "\t-11.6137\t = Validation score   (-root_mean_squared_error)\n",
      "\t31.02s\t = Training   runtime\n",
      "\t0.59s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 200.76s of the 649.53s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.10%)\n",
      "\t-9.0846\t = Validation score   (-root_mean_squared_error)\n",
      "\t161.99s\t = Training   runtime\n",
      "\t0.74s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 37.59s of the 486.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.91%)\n",
      "\t-11.3322\t = Validation score   (-root_mean_squared_error)\n",
      "\t8.26s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 27.26s of the 476.04s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.69%)\n",
      "\t-12.8741\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.22s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 3.61s of the 452.39s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=2.80%)\n",
      "\tTime limit exceeded... Skipping NeuralNetTorch_r79_BAG_L1.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 445.92s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 1.0}\n",
      "\t-9.0846\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 445.85s of the 445.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.93%)\n",
      "2025-11-02 22:46:49,428\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-11-02 22:46:49,430\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-11-02 22:46:49,431\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-11-02 22:46:49,431\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-11-02 22:46:49,432\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-11-02 22:46:49,432\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "2025-11-02 22:46:49,432\tERROR worker.py:420 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n",
      "\t-13.0803\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.05s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 440.11s of the 440.09s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=4.28%)\n",
      "\t-13.0546\t = Validation score   (-root_mean_squared_error)\n",
      "\t3.16s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 435.48s of the 435.46s of remaining time.\n",
      "\t-11.9673\t = Validation score   (-root_mean_squared_error)\n",
      "\t102.44s\t = Training   runtime\n",
      "\t2.37s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 330.04s of the 330.02s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 15.18% memory usage per fold, 60.73%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=15.18%)\n",
      "\t-13.1558\t = Validation score   (-root_mean_squared_error)\n",
      "\t25.4s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 303.54s of the 303.52s of remaining time.\n",
      "\t-10.9953\t = Validation score   (-root_mean_squared_error)\n",
      "\t22.06s\t = Training   runtime\n",
      "\t2.15s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 278.87s of the 278.84s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.97%)\n",
      "\t-12.3834\t = Validation score   (-root_mean_squared_error)\n",
      "\t192.3s\t = Training   runtime\n",
      "\t1.62s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 85.45s of the 85.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.82%)\n",
      "\t-13.057\t = Validation score   (-root_mean_squared_error)\n",
      "\t12.67s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 71.13s of the 71.11s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=3.18%)\n",
      "\t-12.9478\t = Validation score   (-root_mean_squared_error)\n",
      "\t59.0s\t = Training   runtime\n",
      "\t1.12s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 10.85s of the 10.83s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.44%)\n",
      "\t-13.1539\t = Validation score   (-root_mean_squared_error)\n",
      "\t4.76s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 4.02s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetTorch_BAG_L1': 1.0}\n",
      "\t-9.0846\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.13s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1342.28s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 12638.8 rows/s (9293 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/vulcan/Abhay/Projects/ADA/Baseline-2/autogluon_models/task2_forecasting_PRODUCTION\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task 2 Final Results:\n",
      "\n",
      "Leaderboard (Top 5 models):\n",
      "                    model  score_test  score_val  pred_time_test    fit_time\n",
      "0   NeuralNetTorch_BAG_L1    -5.22716  -9.084648        0.715967  161.993685\n",
      "1     WeightedEnsemble_L3    -5.22716  -9.084648        0.717202  162.124785\n",
      "2     WeightedEnsemble_L2    -5.22716  -9.084648        0.717401  162.042589\n",
      "3  NeuralNetFastAI_BAG_L1    -5.25384 -11.255325        1.357575  355.331522\n",
      "4         LightGBM_BAG_L2    -7.18113 -13.054572        7.019010  874.174177\n",
      "\n",
      "Test RMSE: -5.23\n",
      "CV RMSE: -11.41 +/- 9.65\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Check Task 2 Target\n",
    "print(\"Task 2 Target Diagnostics:\")\n",
    "print(f\"Target column: {TARGET_TASK2}\")\n",
    "print(f\"Data type: {X_train_t2[TARGET_TASK2].dtype}\")\n",
    "print(f\"Unique values: {X_train_t2[TARGET_TASK2].nunique()}\")\n",
    "print(f\"Min: {X_train_t2[TARGET_TASK2].min()}\")\n",
    "print(f\"Max: {X_train_t2[TARGET_TASK2].max()}\")\n",
    "print(f\"Mean: {X_train_t2[TARGET_TASK2].mean():.2f}\")\n",
    "print(f\"\\nFirst 10 values:\")\n",
    "print(X_train_t2[TARGET_TASK2].head(10))\n",
    "\n",
    "# Cell 16: Task 2 - Final Model Training (FIXED)\n",
    "predictor_task2 = TabularPredictor(\n",
    "    label=TARGET_TASK2,\n",
    "    path=\"autogluon_models/task2_forecasting_PRODUCTION\",\n",
    "    eval_metric='root_mean_squared_error',\n",
    "    problem_type='regression'  # ← FIX: Force regression\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining final Task 2 model (30 min limit)...\")\n",
    "predictor_task2.fit(\n",
    "    X_train_t2,\n",
    "    presets='best_quality',\n",
    "    time_limit=1800,\n",
    "    ag_args_fit={'num_gpus': 0}\n",
    ")\n",
    "\n",
    "print(f\"\\nTask 2 Final Results:\")\n",
    "leaderboard_t2 = predictor_task2.leaderboard(X_test_t2, silent=True)\n",
    "print(\"\\nLeaderboard (Top 5 models):\")\n",
    "print(leaderboard_t2[['model', 'score_test', 'score_val', 'pred_time_test', 'fit_time']].head())\n",
    "\n",
    "eval_t2_dict = predictor_task2.evaluate(X_test_t2, silent=True)\n",
    "eval_t2 = eval_t2_dict.get('root_mean_squared_error', list(eval_t2_dict.values())[0]) if isinstance(eval_t2_dict, dict) else eval_t2_dict\n",
    "print(f\"\\nTest RMSE: {eval_t2:.2f}\")\n",
    "print(f\"CV RMSE: {np.mean(cv_scores_t2):.2f} +/- {np.std(cv_scores_t2):.2f}\")\n",
    "\n",
    "predictor_task2.save_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16ff18f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TASK 3: SPATIAL INEQUALITY\n",
      "============================================================\n",
      "\n",
      "Pre-split class distribution:\n",
      "high_inequality\n",
      "0    197213\n",
      "1     21878\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Time-based split:\n",
      "  Train: 179,423 samples\n",
      "  Test:  39,668 samples\n",
      "  Train inequality rate: 10.00%\n",
      "  Test inequality rate:  9.94%\n",
      "\n",
      "============================================================\n",
      "TIME-SERIES CV: task3_inequality\n",
      "============================================================\n",
      "\n",
      "Fold 1/3\n",
      "  Train: 44,858 samples | 2025-03-09 to 2025-09-25\n",
      "  Val:   44,855 samples | 2025-03-09 to 2025-09-25\n",
      "  Fold 1 roc_auc: 0.9405\n",
      "\n",
      "Fold 2/3\n",
      "  Train: 89,713 samples | 2025-03-09 to 2025-09-25\n",
      "  Val:   44,855 samples | 2025-03-02 to 2025-09-25\n",
      "  Fold 2 roc_auc: 0.9586\n",
      "\n",
      "Fold 3/3\n",
      "  Train: 134,568 samples | 2025-03-02 to 2025-09-25\n",
      "  Val:   44,855 samples | 2025-03-09 to 2025-09-25\n",
      "  Fold 3 roc_auc: 0.9620\n",
      "\n",
      "task3_inequality CV Results:\n",
      "  Mean roc_auc: 0.9537 +/- 0.0094\n",
      "  Min: 0.9405 | Max: 0.9620\n"
     ]
    }
   ],
   "source": [
    "# Cell 17: Task 3 - Spatial Inequality (CV + Training)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 3: SPATIAL INEQUALITY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_data_task3 = df_task3[FINAL_FEATURES_TASK3 + [TARGET_TASK3]].copy()\n",
    "\n",
    "for cat_col in CATEGORICAL_COLS:\n",
    "    train_data_task3[cat_col] = train_data_task3[cat_col].astype('category')\n",
    "\n",
    "print(f\"\\nPre-split class distribution:\")\n",
    "print(train_data_task3[TARGET_TASK3].value_counts())\n",
    "if train_data_task3[TARGET_TASK3].nunique() < 2:\n",
    "    raise ValueError(\"Only one class present in Task 3!\")\n",
    "\n",
    "X_train_t3 = train_data_task3[df_task3['date'] <= TRAIN_TEST_CUTOFF]\n",
    "X_test_t3 = train_data_task3[df_task3['date'] > TRAIN_TEST_CUTOFF]\n",
    "\n",
    "print(f\"\\nTime-based split:\")\n",
    "print(f\"  Train: {len(X_train_t3):,} samples\")\n",
    "print(f\"  Test:  {len(X_test_t3):,} samples\")\n",
    "print(f\"  Train inequality rate: {X_train_t3[TARGET_TASK3].mean():.2%}\")\n",
    "print(f\"  Test inequality rate:  {X_test_t3[TARGET_TASK3].mean():.2%}\")\n",
    "\n",
    "# Run CV\n",
    "cv_scores_t3 = run_time_series_cv(\n",
    "    X_train_t3,\n",
    "    df_task3.loc[X_train_t3.index, 'date'],\n",
    "    'task3_inequality',\n",
    "    TARGET_TASK3,\n",
    "    'roc_auc',\n",
    "    'binary',\n",
    "    n_splits=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dbab2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"autogluon_models/task3_inequality_PRODUCTION\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.12\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Oct 23 15:35:13 UTC 2025\n",
      "CPU Count:          12\n",
      "Memory Avail:       6.96 GB / 15.25 GB (45.7%)\n",
      "Disk Space Avail:   422.87 GB / 464.17 GB (91.1%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Using hyperparameters preset: hyperparameters='zeroshot'\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 450s of the 1800s of remaining time (25%).\n",
      "\t\tContext path: \"/home/vulcan/Abhay/Projects/ADA/Baseline/autogluon_models/task3_inequality_PRODUCTION/ds_sub_fit/sub_fit_ho\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training final Task 3 model (30 min limit)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0        LightGBMXT_BAG_L2       0.999137   0.998705     roc_auc       14.345190     328.377805  282.658589                 0.380523                2.416536          16.888775            2       True          4\n",
      "1      WeightedEnsemble_L3       0.999073   0.999006     roc_auc       14.687805     335.003201  368.979917                 0.001913                0.016499           3.268507            3       True         10\n",
      "2          CatBoost_BAG_L2       0.999031   0.998966     roc_auc       14.286044     326.393426  331.433492                 0.321377                0.432157          65.663678            2       True          8\n",
      "3    ExtraTreesGini_BAG_L2       0.998992   0.998120     roc_auc       14.064697     328.254058  273.169366                 0.100029                2.292789           7.399553            2       True          9\n",
      "4          LightGBM_BAG_L2       0.998983   0.998663     roc_auc       14.084244     326.399157  272.788896                 0.119576                0.437888           7.019082            2       True          5\n",
      "5      WeightedEnsemble_L2       0.998856   0.998762     roc_auc       13.966833     325.977342  266.636646                 0.002166                0.016073           0.866832            2       True          3\n",
      "6  RandomForestEntr_BAG_L2       0.998854   0.998539     roc_auc       14.124246     328.985489  276.666186                 0.159579                3.024220          10.896372            2       True          7\n",
      "7  RandomForestGini_BAG_L2       0.998836   0.998168     roc_auc       14.104906     329.237535  281.751807                 0.140239                3.276266          15.981993            2       True          6\n",
      "8          LightGBM_BAG_L1       0.998824   0.998610     roc_auc        7.441990     229.874553  109.336631                 7.441990              229.874553         109.336631            1       True          2\n",
      "9        LightGBMXT_BAG_L1       0.998703   0.998604     roc_auc        6.522677      96.086716  156.433182                 6.522677               96.086716         156.433182            1       True          1\n",
      "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t466s\t = DyStack   runtime |\t1334s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=1.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
      "Beginning AutoGluon training ... Time limit = 1334s\n",
      "AutoGluon will save models to \"/home/vulcan/Abhay/Projects/ADA/Baseline/autogluon_models/task3_inequality_PRODUCTION\"\n",
      "Train Data Rows:    179423\n",
      "Train Data Columns: 58\n",
      "Label Column:       high_inequality\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = 0\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    7382.70 MB\n",
      "\tTrain Data (Original)  Memory Usage: 72.23 MB (1.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 3 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUseless Original Features (Count: 2): ['year', 'is_month_end']\n",
      "\t\tThese features carry no predictive signal and should be manually investigated.\n",
      "\t\tThis is typically a feature which has the same value for all rows.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :  3 | ['state', 'district', 'pincode']\n",
      "\t\t('float', [])    : 44 | ['day_sin', 'day_cos', 'month_sin', 'month_cos', 'dow_sin', ...]\n",
      "\t\t('int', [])      :  9 | ['month', 'day', 'day_of_week', 'day_of_year', 'week_of_year', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :  3 | ['state', 'district', 'pincode']\n",
      "\t\t('float', [])     : 44 | ['day_sin', 'day_cos', 'month_sin', 'month_cos', 'dow_sin', ...]\n",
      "\t\t('int', [])       :  6 | ['month', 'day', 'day_of_week', 'day_of_year', 'week_of_year', ...]\n",
      "\t\t('int', ['bool']) :  3 | ['is_weekend', 'is_month_start', 'days_in_month']\n",
      "\t0.4s = Fit runtime\n",
      "\t56 features in original data used to generate 56 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 67.08 MB (0.8% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.5s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'roc_auc'\n",
      "\tThis metric expects predicted probabilities rather than predicted class labels, so you'll need to use predict_proba() instead of predict()\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Large model count detected (110 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
      "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
      "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 888.70s of the 1333.37s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=5.85%)\n",
      "\t0.9987\t = Validation score   (roc_auc)\n",
      "\t159.78s\t = Training   runtime\n",
      "\t202.66s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 705.93s of the 1150.60s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.02%)\n",
      "\t0.9989\t = Validation score   (roc_auc)\n",
      "\t201.16s\t = Training   runtime\n",
      "\t262.21s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 488.27s of the 932.94s of remaining time.\n",
      "\t0.9911\t = Validation score   (roc_auc)\n",
      "\t15.55s\t = Training   runtime\n",
      "\t3.84s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 468.37s of the 913.04s of remaining time.\n",
      "\t0.9919\t = Validation score   (roc_auc)\n",
      "\t13.32s\t = Training   runtime\n",
      "\t3.73s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 450.93s of the 895.61s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 11.36% memory usage per fold, 45.43%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=11.36%)\n",
      "\t0.9977\t = Validation score   (roc_auc)\n",
      "\t363.61s\t = Training   runtime\n",
      "\t1.33s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 85.92s of the 530.60s of remaining time.\n",
      "\t0.9866\t = Validation score   (roc_auc)\n",
      "\t13.96s\t = Training   runtime\n",
      "\t4.47s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 66.95s of the 511.63s of remaining time.\n",
      "\t0.9873\t = Validation score   (roc_auc)\n",
      "\t13.2s\t = Training   runtime\n",
      "\t4.44s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 48.82s of the 493.49s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.57%)\n",
      "\t0.9947\t = Validation score   (roc_auc)\n",
      "\t37.79s\t = Training   runtime\n",
      "\t2.23s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 9.31s of the 453.98s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=7.96%)\n",
      "\t0.9905\t = Validation score   (roc_auc)\n",
      "\t8.03s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 444.16s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM_BAG_L1': 0.944, 'LightGBMXT_BAG_L1': 0.056}\n",
      "\t0.999\t = Validation score   (roc_auc)\n",
      "\t4.15s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting 108 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 439.96s of the 439.93s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.33%)\n",
      "\t0.9991\t = Validation score   (roc_auc)\n",
      "\t18.61s\t = Training   runtime\n",
      "\t2.29s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 419.74s of the 419.71s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=6.40%)\n",
      "\t0.9989\t = Validation score   (roc_auc)\n",
      "\t8.31s\t = Training   runtime\n",
      "\t0.49s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 410.02s of the 409.99s of remaining time.\n",
      "\t0.9988\t = Validation score   (roc_auc)\n",
      "\t20.86s\t = Training   runtime\n",
      "\t3.53s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 385.42s of the 385.40s of remaining time.\n",
      "\t0.999\t = Validation score   (roc_auc)\n",
      "\t12.32s\t = Training   runtime\n",
      "\t3.53s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 369.38s of the 369.35s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 12.18% memory usage per fold, 48.70%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=12.18%)\n",
      "\t0.9993\t = Validation score   (roc_auc)\n",
      "\t296.58s\t = Training   runtime\n",
      "\t0.58s\t = Validation runtime\n",
      "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 71.44s of the 71.41s of remaining time.\n",
      "\t0.9988\t = Validation score   (roc_auc)\n",
      "\t11.41s\t = Training   runtime\n",
      "\t3.91s\t = Validation runtime\n",
      "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 55.85s of the 55.82s of remaining time.\n",
      "\t0.9988\t = Validation score   (roc_auc)\n",
      "\t10.37s\t = Training   runtime\n",
      "\t3.86s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 41.40s of the 41.37s of remaining time.\n",
      "\tMemory not enough to fit 8 folds in parallel. Will train 4 folds in parallel instead (Estimated 10.56% memory usage per fold, 42.23%/80.00% total).\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=10.56%)\n",
      "\t0.9986\t = Validation score   (roc_auc)\n",
      "\t28.58s\t = Training   runtime\n",
      "\t2.41s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 11.34s of the 11.31s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=9.52%)\n",
      "\t0.9986\t = Validation score   (roc_auc)\n",
      "\t9.99s\t = Training   runtime\n",
      "\t0.69s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -0.22s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L2': 0.556, 'RandomForestEntr_BAG_L2': 0.222, 'ExtraTreesGini_BAG_L2': 0.111, 'ExtraTreesEntr_BAG_L2': 0.111}\n",
      "\t0.9993\t = Validation score   (roc_auc)\n",
      "\t8.27s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1342.49s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 47.4 rows/s (22428 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/home/vulcan/Abhay/Projects/ADA/Baseline/autogluon_models/task3_inequality_PRODUCTION\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task 3 Final Results:\n",
      "\n",
      "Leaderboard (Top 5 models):\n",
      "                 model  score_test  score_val  pred_time_test     fit_time\n",
      "0      CatBoost_BAG_L1    0.954865   0.997697        0.754276   363.606946\n",
      "1      CatBoost_BAG_L2    0.951403   0.999259       61.655612  1122.977340\n",
      "2    LightGBMXT_BAG_L1    0.950873   0.998669       25.310220   159.780903\n",
      "3  WeightedEnsemble_L2    0.949935   0.998974       55.135283   365.093319\n",
      "4      LightGBM_BAG_L1    0.947383   0.998894       29.823109   201.160483\n",
      "\n",
      "Test ROC-AUC: 0.9285\n",
      "CV ROC-AUC: 0.9537 +/- 0.0094\n"
     ]
    }
   ],
   "source": [
    "# Cell 18: Task 3 - Final Model Training\n",
    "predictor_task3 = TabularPredictor(\n",
    "    label=TARGET_TASK3,\n",
    "    path=\"autogluon_models/task3_inequality_PRODUCTION\",\n",
    "    eval_metric='roc_auc',\n",
    "    problem_type='binary'\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining final Task 3 model (30 min limit)...\")\n",
    "predictor_task3.fit(\n",
    "    X_train_t3,\n",
    "    presets='best_quality',\n",
    "    time_limit=1800,\n",
    "    ag_args_fit={'num_gpus': 0}\n",
    ")\n",
    "\n",
    "print(f\"\\nTask 3 Final Results:\")\n",
    "leaderboard_t3 = predictor_task3.leaderboard(X_test_t3, silent=True)\n",
    "print(\"\\nLeaderboard (Top 5 models):\")\n",
    "print(leaderboard_t3[['model', 'score_test', 'score_val', 'pred_time_test', 'fit_time']].head())\n",
    "\n",
    "eval_t3_dict = predictor_task3.evaluate(X_test_t3, silent=True)\n",
    "eval_t3 = eval_t3_dict.get('roc_auc', list(eval_t3_dict.values())[0]) if isinstance(eval_t3_dict, dict) else eval_t3_dict\n",
    "print(f\"\\nTest ROC-AUC: {eval_t3:.4f}\")\n",
    "print(f\"CV ROC-AUC: {np.mean(cv_scores_t3):.4f} +/- {np.std(cv_scores_t3):.4f}\")\n",
    "\n",
    "predictor_task3.save_space()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "080310e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PRODUCTION PIPELINE COMPLETE\n",
      "============================================================\n",
      "\n",
      "Data Leak Prevention:\n",
      "  ✓ Task 1: Volatility threshold from training data only\n",
      "  ✓ Task 1: Excluded ALL ratio features derived from aggregates\n",
      "  ✓ Task 2: Date-based split + contemporaneous total_enrollments\n",
      "  ✓ Task 3: Inequality threshold from training data only\n",
      "  ✓ NaN imputation: AutoGluon handles naturally\n",
      "  ✓ All splits: Strictly time-based\n",
      "\n",
      "Final Scores:\n",
      "  Task 1 (Anomaly Detection):\n",
      "    Test ROC-AUC: 0.9652\n",
      "    CV ROC-AUC: 0.9820 +/- 0.0065\n",
      "  Task 2 (7-Day Forecasting):\n",
      "    Test RMSE: -5.23\n",
      "    CV RMSE: -11.41 +/- 9.65\n",
      "  Task 3 (Spatial Inequality):\n",
      "    Test ROC-AUC: 0.9485\n",
      "    CV ROC-AUC: 0.9536 +/- 0.0094\n",
      "\n",
      "============================================================\n",
      "✓ LEAK-FREE BASELINE COMPLETE\n",
      "============================================================\n",
      "Models saved to: autogluon_models/task*_PRODUCTION/\n"
     ]
    }
   ],
   "source": [
    "# Cell 19: Final Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRODUCTION PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nData Leak Prevention:\")\n",
    "print(\"  ✓ Task 1: Volatility threshold from training data only\")\n",
    "print(\"  ✓ Task 1: Excluded ALL ratio features derived from aggregates\")\n",
    "print(\"  ✓ Task 2: Date-based split + contemporaneous total_enrollments\")\n",
    "print(\"  ✓ Task 3: Inequality threshold from training data only\")\n",
    "print(\"  ✓ NaN imputation: AutoGluon handles naturally\")\n",
    "print(\"  ✓ All splits: Strictly time-based\")\n",
    "print(\"\\nFinal Scores:\")\n",
    "print(f\"  Task 1 (Anomaly Detection):\")\n",
    "print(f\"    Test ROC-AUC: {eval_t1:.4f}\")\n",
    "print(f\"    CV ROC-AUC: {np.mean(cv_scores_t1):.4f} +/- {np.std(cv_scores_t1):.4f}\")\n",
    "print(f\"  Task 2 (7-Day Forecasting):\")\n",
    "print(f\"    Test RMSE: {eval_t2:.2f}\")\n",
    "print(f\"    CV RMSE: {np.mean(cv_scores_t2):.2f} +/- {np.std(cv_scores_t2):.2f}\")\n",
    "print(f\"  Task 3 (Spatial Inequality):\")\n",
    "print(f\"    Test ROC-AUC: {eval_t3:.4f}\")\n",
    "print(f\"    CV ROC-AUC: {np.mean(cv_scores_t3):.4f} +/- {np.std(cv_scores_t3):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ LEAK-FREE BASELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"Models saved to: autogluon_models/task*_PRODUCTION/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7d10af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "GENERATING PREDICTION CSV FILES\n",
      "============================================================\n",
      "\n",
      "Task 1 - Anomaly Detection:\n",
      "✓ Saved: task1_anomaly_predictions.csv (39,668 rows)\n",
      "  Accuracy: 96.37%\n",
      "\n",
      "Task 2 - 7-Day Forecasting:\n",
      "✓ Saved: task2_forecasting_predictions.csv (17,971 rows)\n",
      "  RMSE: 5.23\n",
      "  MAE: 2.73\n",
      "  MAPE: 88.50%\n",
      "\n",
      "Task 3 - Spatial Inequality:\n",
      "✓ Saved: task3_inequality_predictions.csv (39,668 rows)\n",
      "  Accuracy: 93.16%\n",
      "\n",
      "============================================================\n",
      "BEST MODELS USED:\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TabularPredictor' object has no attribute 'get_model_best'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBEST MODELS USED:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     72\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m60\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTask 1 Best Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mpredictor_task1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_model_best\u001b[49m()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     74\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTask 2 Best Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictor_task2.get_model_best()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     75\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTask 3 Best Model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredictor_task3.get_model_best()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'TabularPredictor' object has no attribute 'get_model_best'"
     ]
    }
   ],
   "source": [
    "# Cell 20: Generate Prediction CSV Files\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GENERATING PREDICTION CSV FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# === Task 1: Anomaly Detection Predictions ===\n",
    "print(\"\\nTask 1 - Anomaly Detection:\")\n",
    "y_pred_t1 = predictor_task1.predict(X_test_t1)\n",
    "y_pred_proba_t1 = predictor_task1.predict_proba(X_test_t1)\n",
    "\n",
    "# Get test indices and merge with original data\n",
    "test_indices_t1 = X_test_t1.index\n",
    "task1_results = pd.DataFrame({\n",
    "    'date': df_task1.loc[test_indices_t1, 'date'],\n",
    "    'state': df_task1.loc[test_indices_t1, 'state'],\n",
    "    'district': df_task1.loc[test_indices_t1, 'district'],\n",
    "    'pincode': df_task1.loc[test_indices_t1, 'pincode'],\n",
    "    'actual_is_anomaly': X_test_t1[TARGET_TASK1],\n",
    "    'predicted_is_anomaly': y_pred_t1,\n",
    "    'anomaly_probability': y_pred_proba_t1[1] if len(y_pred_proba_t1.shape) > 1 else y_pred_proba_t1\n",
    "})\n",
    "\n",
    "task1_results.to_csv('task1_anomaly_predictions.csv', index=False)\n",
    "print(f\"✓ Saved: task1_anomaly_predictions.csv ({len(task1_results):,} rows)\")\n",
    "print(f\"  Accuracy: {(task1_results['actual_is_anomaly'] == task1_results['predicted_is_anomaly']).mean():.2%}\")\n",
    "\n",
    "# === Task 2: 7-Day Forecasting Predictions ===\n",
    "print(\"\\nTask 2 - 7-Day Forecasting:\")\n",
    "y_pred_t2 = predictor_task2.predict(X_test_t2)\n",
    "\n",
    "test_indices_t2 = X_test_t2.index\n",
    "task2_results = pd.DataFrame({\n",
    "    'date': df_task2.loc[test_indices_t2, 'date'],\n",
    "    'state': df_task2.loc[test_indices_t2, 'state'],\n",
    "    'district': df_task2.loc[test_indices_t2, 'district'],\n",
    "    'pincode': df_task2.loc[test_indices_t2, 'pincode'],\n",
    "    'actual_enrollments_7d': X_test_t2[TARGET_TASK2],\n",
    "    'predicted_enrollments_7d': y_pred_t2,\n",
    "    'absolute_error': abs(X_test_t2[TARGET_TASK2] - y_pred_t2),\n",
    "    'percentage_error': abs((X_test_t2[TARGET_TASK2] - y_pred_t2) / (X_test_t2[TARGET_TASK2] + 1e-10)) * 100\n",
    "})\n",
    "\n",
    "task2_results.to_csv('task2_forecasting_predictions.csv', index=False)\n",
    "print(f\"✓ Saved: task2_forecasting_predictions.csv ({len(task2_results):,} rows)\")\n",
    "print(f\"  RMSE: {np.sqrt(np.mean((task2_results['actual_enrollments_7d'] - task2_results['predicted_enrollments_7d'])**2)):.2f}\")\n",
    "print(f\"  MAE: {task2_results['absolute_error'].mean():.2f}\")\n",
    "print(f\"  MAPE: {task2_results['percentage_error'].mean():.2f}%\")\n",
    "\n",
    "# === Task 3: Spatial Inequality Predictions ===\n",
    "print(\"\\nTask 3 - Spatial Inequality:\")\n",
    "y_pred_t3 = predictor_task3.predict(X_test_t3)\n",
    "y_pred_proba_t3 = predictor_task3.predict_proba(X_test_t3)\n",
    "\n",
    "test_indices_t3 = X_test_t3.index\n",
    "task3_results = pd.DataFrame({\n",
    "    'date': df_task3.loc[test_indices_t3, 'date'],\n",
    "    'state': df_task3.loc[test_indices_t3, 'state'],\n",
    "    'district': df_task3.loc[test_indices_t3, 'district'],\n",
    "    'pincode': df_task3.loc[test_indices_t3, 'pincode'],\n",
    "    'actual_high_inequality': X_test_t3[TARGET_TASK3],\n",
    "    'predicted_high_inequality': y_pred_t3,\n",
    "    'inequality_probability': y_pred_proba_t3[1] if len(y_pred_proba_t3.shape) > 1 else y_pred_proba_t3\n",
    "})\n",
    "\n",
    "task3_results.to_csv('task3_inequality_predictions.csv', index=False)\n",
    "print(f\"✓ Saved: task3_inequality_predictions.csv ({len(task3_results):,} rows)\")\n",
    "print(f\"  Accuracy: {(task3_results['actual_high_inequality'] == task3_results['predicted_high_inequality']).mean():.2%}\")\n",
    "\n",
    "# === Best Model Information ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST MODELS USED:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTask 1 Best Model: {predictor_task1.get_model_best()}\")\n",
    "print(f\"Task 2 Best Model: {predictor_task2.get_model_best()}\")\n",
    "print(f\"Task 3 Best Model: {predictor_task3.get_model_best()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"✓ ALL PREDICTION CSV FILES SAVED\")\n",
    "print(\"=\"*60)\n",
    "print(\"Files created:\")\n",
    "print(\"  - task1_anomaly_predictions.csv\")\n",
    "print(\"  - task2_forecasting_predictions.csv\")\n",
    "print(\"  - task3_inequality_predictions.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
