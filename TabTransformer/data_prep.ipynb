{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cdcb888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. Libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import joblib  # For saving scalers and encoders\n",
    "import json    # For saving model metadata\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "print(\"Setup complete. Libraries loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ae6030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifacts will be saved to: artifacts/\n",
      "Top-K for District: 300\n",
      "Top-K for Pincode: 500\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Constants and File Paths (Corrected)\n",
    "# --- File Paths ---\n",
    "RAW_DATA_FILE = 'cleaned_aadhaar_dataset.csv'\n",
    "ARTIFACTS_DIR = 'artifacts' # This is the folder where all outputs will be saved\n",
    "\n",
    "# --- Preprocessing Parameters ---\n",
    "K_DISTRICT = 300  # Keep the Top 300 most common districts\n",
    "K_PINCODE = 500   # Keep the Top 500 most common pincodes\n",
    "OTHER_TOKEN = '<OTHER>'\n",
    "\n",
    "# --- Feature Definitions (Corrected) ---\n",
    "# We split categories into two types:\n",
    "STRING_CATEGORICAL_FEATURES = ['state', 'district_topK', 'pincode_topK']\n",
    "INTEGER_CATEGORICAL_FEATURES = ['month', 'day_of_week']\n",
    "NUMERICAL_FEATURES = [\n",
    "    'age_0_5', 'age_5_17', 'age_18_greater',\n",
    "    'child_ratio', 'adult_ratio', 'dependent_ratio',\n",
    "    'total_enrollments', 'z_score_state',\n",
    "    'z_score_rolling', 'enrollment_volatility'\n",
    "]\n",
    "\n",
    "# --- Target Definitions ---\n",
    "TARGET_TASK1 = 'is_anomaly'\n",
    "TARGET_TASK2 = 'target_7d'\n",
    "TARGET_TASK3 = 'high_inequality'\n",
    "\n",
    "# Ensure the artifacts directory exists\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "print(f\"Artifacts will be saved to: {ARTIFACTS_DIR}/\")\n",
    "print(f\"Top-K for District: {K_DISTRICT}\")\n",
    "print(f\"Top-K for Pincode: {K_PINCODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e9f18b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data: cleaned_aadhaar_dataset.csv...\n",
      "✓ Data loaded successfully: (219091, 7)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load Raw Data\n",
    "print(f\"Loading raw data: {RAW_DATA_FILE}...\")\n",
    "try:\n",
    "    df = pd.read_csv('/home/vulcan/Abhay/Projects/ADA/Dataset/cleaned_aadhaar_dataset.csv')\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{RAW_DATA_FILE}' not found.\")\n",
    "    raise\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(['state', 'district', 'pincode', 'date']).reset_index(drop=True)\n",
    "print(f\"✓ Data loaded successfully: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9e476bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running feature engineering...\n",
      "✓ All prerequisite and model features created.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Feature Engineering (Prerequisites + Model Features)\n",
    "print(\"Running feature engineering...\")\n",
    "\n",
    "# --- 1. Prerequisites for Targets ---\n",
    "df['total_enrollments'] = df['age_0_5'] + df['age_5_17'] + df['age_18_greater']\n",
    "\n",
    "group_cols = ['state', 'district', 'pincode']\n",
    "df['total_enrollments_rolling_mean_7d'] = df.groupby(group_cols)['total_enrollments'].transform(\n",
    "    lambda x: x.rolling(7, min_periods=1).mean()\n",
    ")\n",
    "df['total_enrollments_rolling_std_7d'] = df.groupby(group_cols)['total_enrollments'].transform(\n",
    "    lambda x: x.rolling(7, min_periods=1).std()\n",
    ")\n",
    "state_stats = df.groupby(['state', 'date']).agg({'total_enrollments': ['mean', 'std']}).reset_index()\n",
    "state_stats.columns = ['state', 'date', 'state_mean', 'state_std']\n",
    "df = df.merge(state_stats, on=['state', 'date'], how='left')\n",
    "df['z_score_state'] = (df['total_enrollments'] - df['state_mean']) / (df['state_std'] + 1e-10)\n",
    "df['z_score_rolling'] = (df['total_enrollments'] - df['total_enrollments_rolling_mean_7d']) / (df['total_enrollments_rolling_std_7d'] + 1e-10)\n",
    "df['enrollment_volatility'] = df['total_enrollments_rolling_std_7d'] / (df['total_enrollments_rolling_mean_7d'] + 1e-10)\n",
    "\n",
    "# --- 2. Features for the TabTransformer Model ---\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['child_ratio'] = df['age_0_5'] / (df['total_enrollments'] + 1e-10)\n",
    "df['adult_ratio'] = df['age_18_greater'] / (df['total_enrollments'] + 1e-10)\n",
    "df['dependent_ratio'] = (df['age_0_5'] + df['age_5_17']) / (df['age_18_greater'] + 1e-10)\n",
    "\n",
    "# --- 3. Sanitize ---\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.fillna(0) # Fill NaNs from rolling/lags\n",
    "print(\"✓ All prerequisite and model features created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d3c9165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating target variables (leak-free)...\n",
      "  Train/Test cutoff date: 2025-09-25\n",
      "✓ Target variables are now in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Target Variable Creation\n",
    "print(\"Creating target variables (leak-free)...\")\n",
    "\n",
    "# Compute train/test cutoff BEFORE creating targets\n",
    "TRAIN_TEST_CUTOFF = df['date'].quantile(0.8)\n",
    "train_mask = df['date'] <= TRAIN_TEST_CUTOFF\n",
    "print(f\"  Train/Test cutoff date: {TRAIN_TEST_CUTOFF.date()}\")\n",
    "\n",
    "# === Task 1: Anomaly Detection ===\n",
    "volatility_threshold = df.loc[train_mask, 'enrollment_volatility'].quantile(0.95)\n",
    "df[TARGET_TASK1] = (\n",
    "    (abs(df['z_score_rolling']) > 2) |\n",
    "    (abs(df['z_score_state']) > 2.5) |\n",
    "    (df['enrollment_volatility'] > volatility_threshold)\n",
    ").astype(int)\n",
    "\n",
    "# === Task 2: 7-Day Forecasting ===\n",
    "df[TARGET_TASK2] = df.groupby(['state', 'district', 'pincode'])['total_enrollments'].shift(-7)\n",
    "\n",
    "# === Task 3: Spatial Inequality ===\n",
    "threshold_inequality = df.loc[train_mask, 'z_score_state'].quantile(0.90)\n",
    "df[TARGET_TASK3] = (df['z_score_state'] > threshold_inequality).astype(int)\n",
    "print(\"✓ Target variables are now in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8548bf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data into train_df and test_df...\n",
      "  Train set shape: (179423, 23)\n",
      "  Test set shape:  (39668, 23)\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Train/Test Split\n",
    "print(\"Splitting data into train_df and test_df...\")\n",
    "train_df = df[train_mask].copy()\n",
    "test_df = df[~train_mask].copy()\n",
    "print(f\"  Train set shape: {train_df.shape}\")\n",
    "print(f\"  Test set shape:  {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57545bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying Top-K cardinality reduction...\n",
      "✓ Cardinality reduction complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Cardinality Reduction (The \"Top-K\" Fix)\n",
    "print(\"Applying Top-K cardinality reduction...\")\n",
    "def apply_top_k(train_series, test_series, k, other_token=OTHER_TOKEN):\n",
    "    top_k_values = train_series.value_counts().index[:k].tolist()\n",
    "    train_mapped = train_series.apply(lambda x: x if x in top_k_values else other_token)\n",
    "    test_mapped = test_series.apply(lambda x: x if x in top_k_values else other_token)\n",
    "    return train_mapped, test_mapped\n",
    "\n",
    "train_df['district_topK'], test_df['district_topK'] = apply_top_k(\n",
    "    train_df['district'], test_df['district'], K_DISTRICT\n",
    ")\n",
    "train_df['pincode_topK'], test_df['pincode_topK'] = apply_top_k(\n",
    "    train_df['pincode'], test_df['pincode'], K_PINCODE\n",
    ")\n",
    "print(\"✓ Cardinality reduction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4d6ae71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting and applying encoders and scalers...\n",
      "  Fitting StandardScaler...\n",
      "  ✓ Numerical features scaled. Scaler saved.\n",
      "  Fitting LabelEncoders for string features...\n",
      "  ✓ String categorical features encoded. Encoders saved.\n",
      "  Calculating cardinalities for integer features...\n",
      "  ✓ Cardinalities saved to JSON: {'state': 37, 'district_topK': 301, 'pincode_topK': 501, 'month': 11, 'day_of_week': 7}\n",
      "✓ Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Preprocessing (Label Encoders & Standard Scalers) (Corrected for JSON)\n",
    "print(\"Fitting and applying encoders and scalers...\")\n",
    "\n",
    "# We will save all fitted objects\n",
    "scalers = {}\n",
    "encoders = {}\n",
    "cardinalities = {} # Metadata for the model architecture\n",
    "\n",
    "# --- 1. Numerical Features ---\n",
    "print(\"  Fitting StandardScaler...\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit ONLY on the training data\n",
    "train_df[NUMERICAL_FEATURES] = scaler.fit_transform(train_df[NUMERICAL_FEATURES])\n",
    "\n",
    "# Transform the test data using the *same* scaler\n",
    "test_df[NUMERICAL_FEATURES] = scaler.transform(test_df[NUMERICAL_FEATURES])\n",
    "\n",
    "# Save the fitted scaler\n",
    "scalers['numerical'] = scaler\n",
    "joblib.dump(scaler, os.path.join(ARTIFACTS_DIR, 'scaler.joblib'))\n",
    "print(\"  ✓ Numerical features scaled. Scaler saved.\")\n",
    "\n",
    "\n",
    "# --- 2. String Categorical Features ---\n",
    "print(\"  Fitting LabelEncoders for string features...\")\n",
    "for col in STRING_CATEGORICAL_FEATURES:\n",
    "    encoder = LabelEncoder()\n",
    "    \n",
    "    unique_vals = train_df[col].unique().tolist()\n",
    "    if OTHER_TOKEN not in unique_vals:\n",
    "        unique_vals.append(OTHER_TOKEN)\n",
    "        \n",
    "    encoder.fit(unique_vals)\n",
    "    \n",
    "    # Transform both train and test\n",
    "    train_df[col] = encoder.transform(train_df[col])\n",
    "    test_df[col] = encoder.transform(test_df[col])\n",
    "    \n",
    "    # Save the encoder and its cardinality (number of unique values)\n",
    "    encoders[col] = encoder\n",
    "    cardinalities[col] = len(encoder.classes_) # len() returns a standard int\n",
    "    \n",
    "joblib.dump(encoders, os.path.join(ARTIFACTS_DIR, 'encoders.joblib'))\n",
    "print(f\"  ✓ String categorical features encoded. Encoders saved.\")\n",
    "\n",
    "# --- 3. Integer Categorical Features (FIXED) ---\n",
    "print(\"  Calculating cardinalities for integer features...\")\n",
    "for col in INTEGER_CATEGORICAL_FEATURES:\n",
    "    max_val = df[col].max() \n",
    "    \n",
    "    # --- THE FIX: Cast to standard Python int ---\n",
    "    cardinalities[col] = int(max_val) + 1 \n",
    "    # ---\n",
    "    \n",
    "# Save the cardinalities as JSON for our PyTorch model\n",
    "with open(os.path.join(ARTIFACTS_DIR, 'cardinalities.json'), 'w') as f:\n",
    "    json.dump(cardinalities, f, indent=4)\n",
    "    \n",
    "print(f\"  ✓ Cardinalities saved to JSON: {cardinalities}\")\n",
    "print(\"✓ Preprocessing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3470cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final processed dataframes...\n",
      "✓ 'train_processed.parquet' and 'test_processed.parquet' saved to 'artifacts'\n",
      "\n",
      "Data preparation for TabTransformer is complete!\n",
      "You can now run the 'Advanced Training' notebook.\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Save Final Processed Data (Corrected)\n",
    "print(\"Saving final processed dataframes...\")\n",
    "\n",
    "# Define the final columns needed for the model and for targeting\n",
    "FINAL_COLS_TO_SAVE = (\n",
    "    STRING_CATEGORICAL_FEATURES +\n",
    "    INTEGER_CATEGORICAL_FEATURES +\n",
    "    NUMERICAL_FEATURES +\n",
    "    [TARGET_TASK1, TARGET_TASK2, TARGET_TASK3]\n",
    ")\n",
    "\n",
    "# Save as Parquet for efficiency and type preservation\n",
    "train_df[FINAL_COLS_TO_SAVE].to_parquet(\n",
    "    os.path.join(ARTIFACTS_DIR, 'train_processed.parquet'), \n",
    "    index=False\n",
    ")\n",
    "test_df[FINAL_COLS_TO_SAVE].to_parquet(\n",
    "    os.path.join(ARTIFACTS_DIR, 'test_processed.parquet'), \n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\"✓ 'train_processed.parquet' and 'test_processed.parquet' saved to '{ARTIFACTS_DIR}'\")\n",
    "print(\"\\nData preparation for TabTransformer is complete!\")\n",
    "print(\"You can now run the 'Advanced Training' notebook.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
